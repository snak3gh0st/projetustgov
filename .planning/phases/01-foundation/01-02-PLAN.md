---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/loader/db_models.py
  - src/loader/database.py
  - src/loader/__init__.py
autonomous: true

must_haves:
  truths:
    - "PostgreSQL contains 4 main tables (propostas, apoiadores, emendas, programas) with correct columns"
    - "Junction tables exist for N:M relationships (proposta_apoiadores, proposta_emendas)"
    - "extraction_logs table tracks each pipeline execution"
    - "All tables have audit columns (created_at, updated_at, extraction_date)"
    - "Unique constraints on transfer_gov_id prevent duplicate records"
    - "Indexes exist on commonly queried fields (status, estado, data_publicacao, valor_total)"
  artifacts:
    - path: "src/loader/db_models.py"
      provides: "SQLAlchemy ORM models for all 7 tables"
      contains: "class Proposta"
    - path: "src/loader/database.py"
      provides: "Engine creation, session factory, schema initialization"
      contains: "create_engine"
  key_links:
    - from: "src/loader/database.py"
      to: "src/config.py"
      via: "database_url from Settings"
      pattern: "get_settings.*database_url"
    - from: "src/loader/db_models.py"
      to: "PostgreSQL"
      via: "SQLAlchemy create_all"
      pattern: "Base\\.metadata\\.create_all"
---

<objective>
Create all SQLAlchemy ORM models for the Transfer Gov data schema and the database connection/session management layer. This establishes the data storage foundation that the loader (Plan 05) will write to and all future queries will read from.

Purpose: The database schema is the backbone of the entire system. All data flows into these tables. Getting the schema right (columns, constraints, indexes, relationships) ensures data integrity and query performance from day one.
Output: 7 SQLAlchemy ORM model classes and a database module that creates the schema in PostgreSQL.
</objective>

<execution_context>
@/Users/pauloloureiro/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pauloloureiro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SQLAlchemy ORM models for all tables</name>
  <files>src/loader/db_models.py</files>
  <action>
    Create `src/loader/db_models.py` with SQLAlchemy 2.0 Mapped API (DeclarativeBase + Mapped + mapped_column).

    Define a `Base` class inheriting from `DeclarativeBase`.

    Create these 7 models:

    **1. Programa (programas)**
    - id: int (primary_key, autoincrement)
    - transfer_gov_id: str (unique, indexed) -- natural key from Transfer Gov
    - nome: Optional[str]
    - orgao_superior: Optional[str]
    - orgao_vinculado: Optional[str]
    - modalidade: Optional[str]
    - acao_orcamentaria: Optional[str]
    - natureza_juridica: Optional[str]
    - Audit: created_at (DateTime, server_default=func.now()), updated_at (DateTime, server_default=func.now(), onupdate=func.now()), extraction_date (Date)

    **2. Proposta (propostas)**
    - id: int (primary_key, autoincrement)
    - transfer_gov_id: str (unique, indexed) -- natural key
    - titulo: Optional[str]
    - valor_global: Optional[float] (Float)
    - valor_repasse: Optional[float] (Float)
    - valor_contrapartida: Optional[float] (Float)
    - data_publicacao: Optional[date] (Date)
    - data_inicio_vigencia: Optional[date]
    - data_fim_vigencia: Optional[date]
    - situacao: Optional[str]
    - estado: Optional[str] (String(2))
    - municipio: Optional[str]
    - proponente: Optional[str]
    - programa_id: Optional[str] -- references programas.transfer_gov_id (application-level FK, NOT database FK -- per RESEARCH.md orphaned record strategy)
    - Audit columns same as above
    - Indexes on: situacao, estado, data_publicacao, valor_global

    **3. Apoiador (apoiadores)**
    - id: int (primary_key, autoincrement)
    - transfer_gov_id: str (unique, indexed)
    - nome: Optional[str]
    - tipo: Optional[str]
    - orgao: Optional[str]
    - Audit columns

    **4. Emenda (emendas)**
    - id: int (primary_key, autoincrement)
    - transfer_gov_id: str (unique, indexed)
    - numero: Optional[str]
    - autor: Optional[str]
    - valor: Optional[float] (Float)
    - tipo: Optional[str]
    - ano: Optional[int]
    - Audit columns

    **5. PropostaApoiador (proposta_apoiadores)** -- junction table
    - id: int (primary_key, autoincrement)
    - proposta_transfer_gov_id: str (indexed)
    - apoiador_transfer_gov_id: str (indexed)
    - UniqueConstraint on (proposta_transfer_gov_id, apoiador_transfer_gov_id)
    - extraction_date: Date

    **6. PropostaEmenda (proposta_emendas)** -- junction table
    - id: int (primary_key, autoincrement)
    - proposta_transfer_gov_id: str (indexed)
    - emenda_transfer_gov_id: str (indexed)
    - UniqueConstraint on (proposta_transfer_gov_id, emenda_transfer_gov_id)
    - extraction_date: Date

    **7. ExtractionLog (extraction_logs)**
    - id: int (primary_key, autoincrement)
    - run_date: datetime (DateTime, server_default=func.now())
    - status: str -- 'success', 'partial', 'failed'
    - files_downloaded: Optional[int]
    - total_records: Optional[int]
    - records_inserted: Optional[int]
    - records_updated: Optional[int]
    - records_skipped: Optional[int]
    - duration_seconds: Optional[float] (Float)
    - error_message: Optional[str] (Text)

    IMPORTANT DESIGN DECISIONS:
    - Use application-level foreign keys (NOT database FK constraints) for proposta.programa_id and junction tables. Per RESEARCH.md discretionary recommendation: "Use DEFERRABLE foreign keys or soft foreign keys (application-level checks)" to handle partial extractions where referenced entities may not exist yet.
    - All Optional fields allow NULL -- per CONTEXT.md: "NOT NULL excluded - allows flexibility for missing data."
    - transfer_gov_id is the natural key for upsert operations (ON CONFLICT target).
    - Column names are in Portuguese to match Transfer Gov source data (reduces mapping confusion).
  </action>
  <verify>
    Run `uv run python -c "from src.loader.db_models import Base, Proposta, Apoiador, Emenda, Programa, PropostaApoiador, PropostaEmenda, ExtractionLog; print(f'Tables: {len(Base.metadata.tables)}'); [print(f'  {t}') for t in Base.metadata.tables]"` -- should list 7 tables.
  </verify>
  <done>7 SQLAlchemy ORM models defined with correct columns, types, constraints, indexes, and audit fields. All models importable from src.loader.db_models.</done>
</task>

<task type="auto">
  <name>Task 2: Create database connection factory and schema initialization</name>
  <files>src/loader/database.py</files>
  <action>
    Create `src/loader/database.py` with:

    1. `create_db_engine(database_url: str) -> Engine`:
       - Uses `create_engine` from SQLAlchemy
       - pool_size=5, max_overflow=10, pool_pre_ping=True (detects stale connections)
       - echo=False (don't log SQL by default)

    2. `create_session_factory(engine: Engine) -> sessionmaker`:
       - Returns `sessionmaker(bind=engine, expire_on_commit=False)`
       - expire_on_commit=False prevents lazy loading issues after commit

    3. `init_db(engine: Engine) -> None`:
       - Calls `Base.metadata.create_all(engine)` to create all tables
       - Logs "Database schema initialized" via Loguru

    4. `get_engine() -> Engine`:
       - Convenience function: reads database_url from get_settings(), creates and caches engine
       - Uses a module-level `_engine` variable with lazy initialization

    5. `get_session() -> Session`:
       - Convenience function: creates session from cached engine
       - Returns a new Session instance (caller responsible for commit/close)

    Import Base from db_models. Import get_settings from config. Import logger from loguru.

    IMPORTANT: Do NOT use async engine. The pipeline is synchronous (single-threaded scheduler). Async adds complexity without benefit for batch ETL.
  </action>
  <verify>
    Ensure docker-compose PostgreSQL is running: `docker compose up -d db`
    Create a .env file if not exists: `cp .env.example .env 2>/dev/null || true`
    Run:
    ```
    uv run python -c "
    from src.loader.database import get_engine, init_db, get_session
    from src.loader.db_models import Base, Proposta, ExtractionLog
    engine = get_engine()
    init_db(engine)
    session = get_session()
    # Verify tables exist by querying
    result = session.execute(session.bind.dialect.has_table(session.bind, 'propostas'))
    session.close()
    print('Schema created successfully')
    "
    ```
    Alternatively, verify with psql:
    ```
    docker compose exec db psql -U projetus -c "\dt"
    ```
    Should show all 7 tables.
  </verify>
  <done>Database engine creates connections to PostgreSQL, session factory produces sessions, init_db creates all 7 tables in the database. Tables verified via psql or SQLAlchemy inspection.</done>
</task>

</tasks>

<verification>
1. All 7 ORM models importable from src.loader.db_models
2. Base.metadata.tables contains exactly 7 tables
3. `docker compose exec db psql -U projetus -c "\dt"` shows all tables
4. `docker compose exec db psql -U projetus -c "\d propostas"` shows correct columns with types
5. Unique constraint on transfer_gov_id verified: `docker compose exec db psql -U projetus -c "\d propostas" | grep unique`
6. Indexes verified: `docker compose exec db psql -U projetus -c "\di" | grep ix_`
</verification>

<success_criteria>
- 7 tables created in PostgreSQL matching the Transfer Gov data model
- Unique constraints on transfer_gov_id columns (required for upsert)
- Indexes on commonly queried fields (situacao, estado, data_publicacao, valor_global)
- Audit columns (created_at, updated_at, extraction_date) on all entity tables
- Session factory produces working sessions that can query the database
- No database-level foreign key constraints (application-level only, supporting partial extractions)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
