---
phase: 01-foundation
plan: 03
type: tdd
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/parser/__init__.py
  - src/parser/encoding.py
  - src/parser/schemas.py
  - src/parser/file_parser.py
  - src/transformer/__init__.py
  - src/transformer/models.py
  - src/transformer/validator.py
  - tests/test_parser.py
  - tests/test_validator.py
  - tests/fixtures/sample_propostas.xlsx
  - tests/fixtures/sample_propostas_latin1.csv
autonomous: true

must_haves:
  truths:
    - "Encoding detection correctly identifies UTF-8, UTF-8-sig, Windows-1252, and ISO-8859-1 files"
    - "Parser reads Excel (.xlsx) files and returns Polars DataFrames with correct Portuguese characters"
    - "Parser reads CSV files with auto-detected encoding and returns Polars DataFrames"
    - "Pydantic validator accepts valid Transfer Gov records and returns validated models"
    - "Pydantic validator rejects invalid records (empty IDs, negative values) with clear error messages"
    - "Schema validation detects missing required columns before processing"
  artifacts:
    - path: "src/parser/encoding.py"
      provides: "Encoding auto-detection with fallback chain"
      contains: "detect_encoding"
    - path: "src/parser/file_parser.py"
      provides: "Excel and CSV parsing via Polars"
      contains: "parse_file"
    - path: "src/parser/schemas.py"
      provides: "Expected column definitions per file type"
      contains: "EXPECTED_COLUMNS"
    - path: "src/transformer/models.py"
      provides: "Pydantic validation models for each entity"
      contains: "class PropostaValidation"
    - path: "src/transformer/validator.py"
      provides: "Batch validation of DataFrames against Pydantic models"
      contains: "validate_dataframe"
    - path: "tests/test_parser.py"
      provides: "Parser tests with fixture files"
      contains: "test_parse"
    - path: "tests/test_validator.py"
      provides: "Validator tests for valid and invalid records"
      contains: "test_valid"
  key_links:
    - from: "src/parser/encoding.py"
      to: "charset-normalizer"
      via: "from_path for encoding detection"
      pattern: "from_path"
    - from: "src/parser/file_parser.py"
      to: "polars"
      via: "read_excel and read_csv"
      pattern: "pl\\.read_(excel|csv)"
    - from: "src/transformer/validator.py"
      to: "src/transformer/models.py"
      via: "Pydantic model_validate for each row"
      pattern: "model_validate"
---

<objective>
Build and test the file parsing and data validation pipeline using TDD. This covers encoding detection (charset-normalizer), Excel/CSV parsing (Polars), schema validation (expected columns), and record-level validation (Pydantic models). TDD ensures correctness of data transformations before integration with the rest of the pipeline.

Purpose: Parser and validator are the data quality gatekeepers. If encoding is wrong, Portuguese characters get corrupted. If validation is weak, bad data enters the database silently. TDD forces us to define correct behavior BEFORE implementation.
Output: Tested parser module (encoding + file reading + schema check) and tested validator module (Pydantic models for all 4 entity types).
</objective>

<execution_context>
@/Users/pauloloureiro/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pauloloureiro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-CONTEXT.md
</context>

<feature>
  <name>File Parser + Data Validator</name>
  <files>
    src/parser/encoding.py
    src/parser/schemas.py
    src/parser/file_parser.py
    src/transformer/models.py
    src/transformer/validator.py
    tests/test_parser.py
    tests/test_validator.py
    tests/fixtures/
  </files>
  <behavior>
    **Encoding Detection:**
    - detect_encoding("utf8_file.csv") -> "utf8"
    - detect_encoding("latin1_file.csv") -> "windows-1252" (normalized)
    - detect_encoding("unknown_file.csv") -> "utf8" (safe fallback)

    **File Parsing:**
    - parse_file("propostas.xlsx", "propostas") -> Polars DataFrame with correct columns
    - parse_file("propostas.csv", "propostas") -> Polars DataFrame with auto-detected encoding
    - parse_file("propostas.xlsx", "propostas") with missing required columns -> raises SchemaValidationError
    - parse_file("empty.xlsx", "propostas") -> raises EmptyFileError

    **Schema Validation:**
    - EXPECTED_COLUMNS["propostas"] = ["transfer_gov_id", "titulo", "valor_global", ...]
    - validate_schema(df, "propostas") -> True if required columns present
    - validate_schema(df, "propostas") -> raises SchemaValidationError listing missing columns

    **Record Validation (Pydantic):**
    - PropostaValidation(transfer_gov_id="123", titulo="Test", ...) -> valid model
    - PropostaValidation(transfer_gov_id="", ...) -> ValidationError("transfer_gov_id cannot be empty")
    - PropostaValidation(valor_global=-100, ...) -> ValidationError("valor cannot be negative")
    - PropostaValidation(estado="XX", ...) -> ValidationError("Invalid estado UF")
    - validate_dataframe(df, "propostas") -> (valid_records: list[dict], errors: list[dict])
  </behavior>
  <implementation>
    **RED phase -- Write tests first:**

    1. Create test fixture files in tests/fixtures/:
       - `sample_propostas.xlsx`: Small Excel file with 5 rows of valid proposta data (use openpyxl to create programmatically in a conftest.py or setup fixture). Columns: transfer_gov_id, titulo, valor_global, valor_repasse, valor_contrapartida, situacao, estado, municipio, proponente, programa_id
       - `sample_propostas_latin1.csv`: Same data as CSV but saved in Windows-1252 encoding with Portuguese characters (acentuacao, cedilha)
       - `sample_apoiadores.xlsx`: 3 rows with transfer_gov_id, nome, tipo, orgao
       - `sample_emendas.xlsx`: 3 rows with transfer_gov_id, numero, autor, valor, tipo, ano
       - `sample_programas.xlsx`: 3 rows with transfer_gov_id, nome, orgao_superior, modalidade

    2. Write tests/test_parser.py:
       - test_detect_encoding_utf8: verify UTF-8 file detected correctly
       - test_detect_encoding_latin1: verify Windows-1252/Latin-1 file detected and normalized
       - test_parse_excel_propostas: parse sample .xlsx, assert DataFrame shape, column names, Portuguese chars intact
       - test_parse_csv_with_encoding: parse Latin-1 CSV, assert Portuguese chars render correctly
       - test_parse_missing_columns: parse file missing required columns, assert SchemaValidationError raised
       - test_parse_empty_file: parse empty file, assert EmptyFileError raised

    3. Write tests/test_validator.py:
       - test_valid_proposta: valid record passes validation
       - test_empty_id_rejected: empty transfer_gov_id raises error
       - test_negative_valor_rejected: negative valor_global raises error
       - test_invalid_estado_rejected: invalid UF code raises error
       - test_valid_apoiador: valid apoiador passes
       - test_valid_emenda: valid emenda passes
       - test_valid_programa: valid programa passes
       - test_validate_dataframe_mixed: DataFrame with mix of valid/invalid rows returns correct split

    **GREEN phase -- Implement to pass:**

    1. `src/parser/encoding.py`:
       - `detect_encoding(file_path: str) -> str`: Use charset_normalizer.from_path(), normalize encoding names (ascii->utf8, iso-8859-1->windows-1252, latin-1->windows-1252, cp1252->windows-1252), fallback to "utf8"

    2. `src/parser/schemas.py`:
       - `EXPECTED_COLUMNS: dict[str, list[str]]` mapping file_type to required column names
       - `validate_schema(df: pl.DataFrame, file_type: str) -> None`: Check all required columns present, raise SchemaValidationError with list of missing columns
       - Custom exceptions: `SchemaValidationError`, `EmptyFileError`

    3. `src/parser/file_parser.py`:
       - `parse_file(file_path: str, file_type: str) -> pl.DataFrame`:
         - Detect file extension (.xlsx vs .csv)
         - For .xlsx: use `pl.read_excel(file_path, engine="openpyxl")`
         - For .csv: detect encoding first, then `pl.read_csv(file_path, encoding=detected)`
         - Check DataFrame is not empty (raise EmptyFileError)
         - Validate schema (raise SchemaValidationError)
         - Log columns found and row count
         - Return DataFrame

       IMPORTANT: Always use pl.read_csv (eager mode), NOT pl.scan_csv -- per RESEARCH.md Pitfall 1, scan_csv only supports UTF-8.
       IMPORTANT: For schema validation, normalize column names (strip whitespace, lowercase) before comparing. Transfer Gov exports may have inconsistent casing.

    4. `src/transformer/models.py`:
       - PropostaValidation(BaseModel): transfer_gov_id (str, required), titulo (Optional[str]), valor_global (Optional[float]), valor_repasse (Optional[float]), valor_contrapartida (Optional[float]), data_publicacao (Optional[date]), data_inicio_vigencia (Optional[date]), data_fim_vigencia (Optional[date]), situacao (Optional[str]), estado (Optional[str]), municipio (Optional[str]), proponente (Optional[str]), programa_id (Optional[str])
         - field_validator on transfer_gov_id: strip whitespace, reject empty
         - field_validator on valor_global/valor_repasse/valor_contrapartida: reject negative
         - field_validator on estado: validate against 27 UF codes, allow None
       - ApoiadorValidation(BaseModel): transfer_gov_id (str, required), nome (Optional[str]), tipo (Optional[str]), orgao (Optional[str])
       - EmendaValidation(BaseModel): transfer_gov_id (str, required), numero (Optional[str]), autor (Optional[str]), valor (Optional[float]), tipo (Optional[str]), ano (Optional[int])
       - ProgramaValidation(BaseModel): transfer_gov_id (str, required), nome (Optional[str]), orgao_superior (Optional[str]), orgao_vinculado (Optional[str]), modalidade (Optional[str]), acao_orcamentaria (Optional[str]), natureza_juridica (Optional[str])

    5. `src/transformer/validator.py`:
       - `VALIDATION_MODELS: dict[str, type[BaseModel]]` mapping file_type to Pydantic model
       - `validate_dataframe(df: pl.DataFrame, file_type: str) -> tuple[list[dict], list[dict]]`:
         - Iterate over DataFrame rows (df.iter_rows(named=True))
         - For each row, try model_validate(row_dict)
         - Valid rows: append model.model_dump() to valid_records
         - Invalid rows: append {"row": row_dict, "errors": str(validation_error)} to errors
         - Log summary: "{N} valid, {M} invalid out of {total} rows"
         - Return (valid_records, errors)

    NOTE ON COLUMN NAMES: The exact column names from Transfer Gov are unknown until we download actual files (RESEARCH.md Open Question 1). Use reasonable Portuguese column names based on the domain. The parser normalizes column names (lowercase, strip), and the schema mapping can be updated when real files are inspected during Plan 04 (crawler). This is an expected iteration point, NOT a blocker.
  </implementation>
</feature>

<verification>
1. `uv run pytest tests/test_parser.py -v` -- all tests pass
2. `uv run pytest tests/test_validator.py -v` -- all tests pass
3. Fixture files exist in tests/fixtures/ with Portuguese characters
4. Encoding detection correctly identifies test fixture encodings
5. Invalid records produce clear error messages (not generic exceptions)
</verification>

<success_criteria>
- All parser tests pass (encoding detection, Excel parsing, CSV parsing, schema validation, empty file detection)
- All validator tests pass (valid records accepted, invalid records rejected with clear messages, mixed DataFrame handled correctly)
- Portuguese characters preserved through entire parse -> validate pipeline
- Schema validation catches missing columns before processing
- validate_dataframe returns clean split of valid vs invalid records with error details
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
