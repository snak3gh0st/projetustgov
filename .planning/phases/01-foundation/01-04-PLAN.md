---
phase: 01-foundation
plan: 04
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/crawler/__init__.py
  - src/crawler/browser.py
  - src/crawler/navigator.py
  - src/crawler/downloader.py
autonomous: false

must_haves:
  truths:
    - "Playwright launches Chromium browser and navigates to Transfer Gov panel"
    - "Crawler downloads files from Transfer Gov and saves to data/raw/YYYY-MM-DD/"
    - "Retry logic retries failed downloads 3 times with exponential backoff (2s, 4s, 8s)"
    - "Failed file downloads are logged but do not halt other file downloads (partial extraction)"
    - "Raw files preserved on disk before any processing occurs"
  artifacts:
    - path: "src/crawler/browser.py"
      provides: "Playwright browser lifecycle management"
      contains: "class BrowserManager"
    - path: "src/crawler/navigator.py"
      provides: "Transfer Gov page navigation and export triggering"
      contains: "navigate_to_panel"
    - path: "src/crawler/downloader.py"
      provides: "File download orchestration with retry and raw file storage"
      contains: "download_files"
  key_links:
    - from: "src/crawler/browser.py"
      to: "playwright"
      via: "sync_playwright context manager"
      pattern: "sync_playwright"
    - from: "src/crawler/downloader.py"
      to: "data/raw/"
      via: "os.makedirs and download.save_as"
      pattern: "save_as"
    - from: "src/crawler/downloader.py"
      to: "tenacity"
      via: "retry decorator with exponential backoff"
      pattern: "@retry"
---

<objective>
Build the web crawler that uses Playwright to navigate Transfer Gov, trigger file exports, and download the 4 data files (propostas, apoiadores, emendas, programas) to local storage. Includes retry logic, timeout handling, and raw file preservation. Ends with a human verification checkpoint since Transfer Gov site structure needs runtime validation.

Purpose: The crawler is the data acquisition layer. Without reliable file downloads, nothing downstream works. Retry logic and partial extraction support ensure the pipeline is resilient to Transfer Gov's government-hosted infrastructure.
Output: Working crawler that downloads files from Transfer Gov to data/raw/YYYY-MM-DD/, with retry and partial failure handling.
</objective>

<execution_context>
@/Users/pauloloureiro/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pauloloureiro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create browser manager and Transfer Gov navigator</name>
  <files>
    src/crawler/browser.py
    src/crawler/navigator.py
  </files>
  <action>
    1. Create `src/crawler/browser.py` with a `BrowserManager` class:
       - Context manager (supports `with BrowserManager() as bm:`)
       - `__enter__`: Launch Playwright sync, create Chromium browser (headless=True), create browser context with long timeout
       - `__exit__`: Close browser context, close browser, stop Playwright
       - `new_page() -> Page`: Create new page with custom timeout (120_000ms = 2 minutes default for government portals)
       - Accept `headless: bool = True` parameter (allows headed mode for debugging)
       - Log browser launch and close events

    2. Create `src/crawler/navigator.py` with functions:
       - `navigate_to_panel(page: Page, url: str) -> None`:
         - Navigate to Transfer Gov URL with generous timeout (60_000ms)
         - Wait for page to be fully loaded (wait for networkidle or specific selector)
         - Log navigation success
         - NOTE: The exact Transfer Gov panel is a Qlik Sense dashboard (per RESEARCH.md). The page may take time to render. Use `page.wait_for_load_state("networkidle")` with a long timeout.

       - `find_export_buttons(page: Page) -> list[dict]`:
         - Locate export/download buttons on the Transfer Gov panel
         - Try multiple selector strategies (CSS > XPath > text-based) per EXTR-04
         - Return list of dicts: [{"file_type": "propostas", "selector": "..."}, ...]
         - If no export buttons found, raise a `NavigationError` with details about what was expected vs found
         - Log selectors found

       - Custom exception: `NavigationError`

       IMPORTANT: Since we don't know the exact selectors yet (RESEARCH.md Open Question 1), implement the navigator with a FLEXIBLE selector approach:
       - Define a `SELECTORS` dict at module level with multiple fallback selectors per file type
       - Each entry: {"primary": "css_selector", "fallback_xpath": "//xpath", "fallback_text": "text content"}
       - `find_export_buttons` tries each strategy in order
       - Use placeholder selectors that are clearly marked as "NEEDS_UPDATE_FROM_SITE_INSPECTION"
       - The checkpoint task below will validate and update these selectors

       This approach means the code structure is production-ready even though selectors need runtime validation.
  </action>
  <verify>
    Run `uv run python -c "from src.crawler.browser import BrowserManager; from src.crawler.navigator import navigate_to_panel; print('Crawler imports OK')"` -- should succeed.
    Run `uv run python -c "
    from src.crawler.browser import BrowserManager
    with BrowserManager(headless=True) as bm:
        page = bm.new_page()
        page.goto('https://example.com')
        print(f'Title: {page.title()}')
    print('Browser lifecycle OK')
    "` -- should print a page title and confirm browser opens/closes cleanly.
  </verify>
  <done>BrowserManager launches Playwright Chromium with proper lifecycle management. Navigator module has flexible selector strategy for Transfer Gov panel. Browser opens and closes cleanly.</done>
</task>

<task type="auto">
  <name>Task 2: Create file downloader with retry logic and raw file storage</name>
  <files>
    src/crawler/downloader.py
  </files>
  <action>
    Create `src/crawler/downloader.py` with:

    1. `get_raw_dir(extraction_date: date | None = None) -> str`:
       - Create and return date-organized directory: `data/raw/YYYY-MM-DD/`
       - Use os.makedirs(exist_ok=True)
       - Default to today's date

    2. `cleanup_old_raw_files(raw_data_dir: str, retention_days: int) -> None`:
       - List date-named subdirectories in raw_data_dir
       - Delete directories older than retention_days
       - Log each deletion

    3. `download_single_file(page: Page, selector: str, save_dir: str, file_type: str) -> str`:
       - Decorated with `@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=2, min=2, max=8), retry=retry_if_exception_type((TimeoutError, Exception)), before_sleep=log_retry)`
       - Use `page.expect_download(timeout=300_000)` (5 minutes for government report generation)
       - Click the selector to trigger download
       - Save downloaded file to save_dir with a clean filename: `{file_type}.xlsx` or `{file_type}.csv` (based on download.suggested_filename extension)
       - Log download success with file size
       - Return saved file path

    4. `download_all_files(page: Page, export_buttons: list[dict], save_dir: str) -> dict[str, str | None]`:
       - Iterate over export_buttons
       - For each, call download_single_file in try/except
       - On success: store file_type -> file_path in results
       - On failure after all retries: log error, store file_type -> None in results (partial extraction per CONTEXT.md decision)
       - Return results dict: {"propostas": "/path/to/file.xlsx", "apoiadores": None, ...}
       - Log summary: "Downloaded X/4 files"

    5. `run_crawler(settings: Settings) -> dict[str, str | None]`:
       - High-level function that orchestrates the full crawl:
         1. Create raw directory for today
         2. Cleanup old raw files
         3. Open BrowserManager
         4. Navigate to Transfer Gov panel
         5. Find export buttons
         6. Download all files
         7. Return results dict
       - Wrap everything in try/except, log errors with full traceback
       - On NavigationError: return empty dict and log the error

    IMPORTANT: The retry decorator is on individual file downloads, NOT on the entire crawl. Per CONTEXT.md: "Skip that file, process others."
    IMPORTANT: Use tenacity (not custom retry logic) per RESEARCH.md "Don't Hand-Roll" section.
  </action>
  <verify>
    Run `uv run python -c "from src.crawler.downloader import get_raw_dir, cleanup_old_raw_files, download_all_files, run_crawler; print('Downloader imports OK')"` -- should succeed.
    Test raw directory creation: `uv run python -c "from src.crawler.downloader import get_raw_dir; d = get_raw_dir(); print(f'Dir: {d}'); import os; print(f'Exists: {os.path.isdir(d)}')"` -- should print today's directory and True.
  </verify>
  <done>Downloader creates date-organized raw file directories, retries failed downloads 3 times with exponential backoff, handles partial failures gracefully, and cleans up old files. Full crawl orchestrated by run_crawler().</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete crawler module with Playwright browser management, Transfer Gov navigation, file download with retry logic, and raw file storage. The crawler is structurally complete but uses PLACEHOLDER selectors for Transfer Gov because the exact page structure needs runtime inspection.
  </what-built>
  <how-to-verify>
    1. Run the crawler in headed (non-headless) mode to inspect Transfer Gov:
       ```
       uv run python -c "
       from src.crawler.browser import BrowserManager
       from src.crawler.navigator import navigate_to_panel
       from src.config import get_settings
       settings = get_settings()
       with BrowserManager(headless=False) as bm:
           page = bm.new_page()
           navigate_to_panel(page, settings.transfer_gov_url)
           input('Press Enter after inspecting the page...')
       "
       ```
    2. Observe the Transfer Gov panel. Look for:
       - Export/download buttons (likely labeled "Exportar", "Download", or an icon)
       - How many files can be downloaded and in what format
       - Whether the page requires any interaction before export buttons appear
    3. Note the actual selectors and file structure observed
    4. Confirm whether the panel matches expectations from RESEARCH.md (Qlik Sense dashboard with export capability)
  </how-to-verify>
  <resume-signal>
    Type "approved" with any selector/structure notes, OR describe issues found.
    If Transfer Gov structure differs significantly from expectations, describe what you see and the plan will be adapted.
  </resume-signal>
</task>

</tasks>

<verification>
1. BrowserManager opens and closes Playwright Chromium cleanly
2. Navigator module has flexible selector strategy (primary + fallback)
3. Downloader creates data/raw/YYYY-MM-DD/ directories
4. Retry logic configured: 3 attempts, exponential backoff (2s, 4s, 8s)
5. Partial failure handling: failed files logged, other files still processed
6. run_crawler() orchestrates the full crawl sequence
7. Human verified Transfer Gov page structure and selectors
</verification>

<success_criteria>
- Crawler launches Playwright, navigates to Transfer Gov, and can download files
- Failed downloads retry 3 times before giving up (per file, not entire crawl)
- Partial extraction supported: failed files don't block successful ones
- Raw files saved to date-organized directories (data/raw/YYYY-MM-DD/)
- Old raw files cleaned up based on retention policy
- Transfer Gov page structure validated by human inspection
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-04-SUMMARY.md`
</output>
