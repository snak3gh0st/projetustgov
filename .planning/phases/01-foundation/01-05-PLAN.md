---
phase: 01-foundation
plan: 05
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - src/loader/upsert.py
  - src/loader/extraction_log.py
  - tests/test_loader.py
autonomous: true

must_haves:
  truths:
    - "Upsert inserts new records and updates existing records on transfer_gov_id conflict"
    - "All database writes for one extraction run are wrapped in a single atomic transaction"
    - "If any table write fails, the entire extraction rolls back (no partial data in DB)"
    - "Re-running the same extraction does not create duplicate records"
    - "ExtractionLog records every pipeline run with status, row counts, duration, and errors"
    - "Valid rows are loaded even when some rows failed validation (per CONTEXT.md partial load decision)"
  artifacts:
    - path: "src/loader/upsert.py"
      provides: "Bulk upsert with ON CONFLICT DO UPDATE for all entity tables"
      contains: "upsert_records"
    - path: "src/loader/extraction_log.py"
      provides: "ExtractionLog creation and status tracking"
      contains: "create_extraction_log"
    - path: "tests/test_loader.py"
      provides: "Loader tests: upsert idempotency, atomic transactions, extraction logging"
      contains: "test_upsert"
  key_links:
    - from: "src/loader/upsert.py"
      to: "src/loader/db_models.py"
      via: "SQLAlchemy table references for ON CONFLICT"
      pattern: "insert.*on_conflict_do_update"
    - from: "src/loader/upsert.py"
      to: "sqlalchemy.dialects.postgresql"
      via: "PostgreSQL-specific insert for upsert"
      pattern: "from sqlalchemy\\.dialects\\.postgresql import insert"
    - from: "src/loader/extraction_log.py"
      to: "src/loader/db_models.py"
      via: "ExtractionLog model for run tracking"
      pattern: "ExtractionLog"
---

<objective>
Build the data loader: bulk upsert operations using PostgreSQL ON CONFLICT DO UPDATE, atomic transaction management, and extraction run logging. This connects validated data (from Plan 03) to the database schema (from Plan 02).

Purpose: Upsert guarantees idempotency (re-runs don't duplicate data). Atomic transactions guarantee consistency (no partial data on failure). Extraction logging provides audit trail and operational visibility. These are the core zero-data-loss mechanisms.
Output: Upsert module, extraction logger, and tests proving idempotency and atomicity.
</objective>

<execution_context>
@/Users/pauloloureiro/.claude/get-shit-done/workflows/execute-plan.md
@/Users/pauloloureiro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-CONTEXT.md
@.planning/phases/01-foundation/01-02-SUMMARY.md
@.planning/phases/01-foundation/01-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create upsert module and extraction logger</name>
  <files>
    src/loader/upsert.py
    src/loader/extraction_log.py
  </files>
  <action>
    1. Create `src/loader/upsert.py` with:

       `upsert_records(session: Session, model_class, records: list[dict], conflict_column: str = "transfer_gov_id") -> dict`:
       - If records is empty, return {"inserted": 0, "updated": 0}
       - Use `from sqlalchemy.dialects.postgresql import insert`
       - Build insert statement: `insert(model_class.__table__).values(records)`
       - Build update dict: all columns EXCEPT id (primary key) and the conflict_column. Always include updated_at.
       - Add `.on_conflict_do_update(index_elements=[conflict_column], set_=update_cols)`
       - Execute via session.execute(stmt)
       - Return {"inserted": result.rowcount, "updated": 0} (PostgreSQL doesn't distinguish insert vs update in rowcount, but rowcount gives total affected)
       - Log: "Upserted {rowcount} records into {table_name}"

       `load_extraction_data(session: Session, validated_data: dict[str, list[dict]], extraction_date: date) -> dict`:
       - Add extraction_date to each record in all lists
       - Load tables in dependency order: programas first, then propostas, then apoiadores, then emendas, then junction tables (proposta_apoiadores, proposta_emendas)
       - For junction tables, use compound unique constraint columns as conflict target
       - Aggregate stats: total inserted/updated per table
       - Return stats dict: {"programas": {"count": N}, "propostas": {"count": N}, ...}
       - Do NOT commit -- caller manages transaction boundary

       IMPORTANT: `load_extraction_data` does NOT commit the session. The atomic transaction boundary is managed by the orchestrator (Plan 06). This separation ensures the entire extraction is atomic.

       IMPORTANT: For junction tables (PropostaApoiador, PropostaEmenda), the conflict target is the compound unique constraint columns (proposta_transfer_gov_id, apoiador_transfer_gov_id) or (proposta_transfer_gov_id, emenda_transfer_gov_id). Use `index_elements` with both columns.

    2. Create `src/loader/extraction_log.py` with:

       `create_extraction_log(session: Session, status: str, stats: dict | None = None, error: str | None = None, duration: float | None = None) -> ExtractionLog`:
       - Create ExtractionLog instance with provided values
       - Populate: files_downloaded, total_records, records_inserted, records_updated, records_skipped from stats dict
       - session.add(log_entry)
       - session.flush() (make ID available but don't commit)
       - Return the log entry
       - Log: "Extraction log created: status={status}, records={total}"

       `get_last_extraction(session: Session) -> ExtractionLog | None`:
       - Query extraction_logs ordered by run_date desc, limit 1
       - Return the most recent log entry or None
       - Used by health check endpoint (Plan 06)
  </action>
  <verify>
    Run `uv run python -c "from src.loader.upsert import upsert_records, load_extraction_data; from src.loader.extraction_log import create_extraction_log, get_last_extraction; print('Loader imports OK')"` -- should succeed.
  </verify>
  <done>Upsert module handles bulk inserts with ON CONFLICT DO UPDATE for all tables. Extraction logger creates audit trail entries. Both modules delegate transaction management to caller.</done>
</task>

<task type="auto">
  <name>Task 2: Create integration tests for loader (idempotency + atomicity)</name>
  <files>tests/test_loader.py</files>
  <action>
    Create `tests/test_loader.py` with integration tests that use the real PostgreSQL database (via docker-compose).

    Add a `conftest.py` fixture (or use tests/conftest.py) that:
    - Creates a test database engine from DATABASE_URL (or use the same dev database)
    - Creates all tables via init_db()
    - Yields a session
    - After test: rolls back and cleans up (truncate tables or use transaction rollback)

    Tests:

    1. `test_upsert_insert_new_records`:
       - Insert 3 new propostas via upsert_records
       - Query propostas table, assert count == 3
       - Assert all fields match input

    2. `test_upsert_update_existing_records`:
       - Insert 2 propostas
       - Upsert same 2 records with updated titulo
       - Assert count is still 2 (not 4 -- no duplicates)
       - Assert titulo was updated to new value
       - Assert updated_at changed

    3. `test_upsert_idempotent`:
       - Insert 3 propostas
       - Re-insert exact same 3 records
       - Assert count is still 3

    4. `test_load_extraction_data_ordering`:
       - Create validated_data dict with programas, propostas, apoiadores, emendas
       - Call load_extraction_data
       - Commit session
       - Query all tables, verify data present

    5. `test_atomic_rollback_on_failure`:
       - Start session
       - Insert some propostas via upsert
       - Simulate failure (e.g., insert invalid data for a different table)
       - Rollback session
       - Verify NO data was written to any table

    6. `test_extraction_log_created`:
       - Call create_extraction_log with status="success" and stats
       - Commit session
       - Call get_last_extraction
       - Assert returned log matches

    IMPORTANT: Tests require PostgreSQL running via docker-compose. Add a pytest marker `@pytest.mark.integration` and skip if DATABASE_URL not available:
    ```python
    import pytest
    import os
    pytestmark = pytest.mark.skipif(
        not os.getenv("DATABASE_URL"),
        reason="DATABASE_URL not set, skip integration tests"
    )
    ```

    Ensure .env is loaded in conftest.py (use python-dotenv load_dotenv).
  </action>
  <verify>
    Ensure PostgreSQL is running: `docker compose up -d db`
    Run tests: `uv run pytest tests/test_loader.py -v` -- all tests should pass.
    Verify idempotency: run tests twice -- second run should also pass (no leftover state).
  </verify>
  <done>Integration tests prove: upsert inserts new records, updates existing records, prevents duplicates (idempotent), rolls back atomically on failure, and extraction logs track each run. All tests pass against real PostgreSQL.</done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_loader.py -v` -- all 6 tests pass
2. Upsert inserts new records: count increases
3. Upsert updates existing records: count unchanged, values updated
4. Re-running same data: count unchanged (idempotent)
5. Rollback test: no partial data after simulated failure
6. Extraction log captures status, counts, duration
</verification>

<success_criteria>
- upsert_records correctly uses ON CONFLICT DO UPDATE with transfer_gov_id
- load_extraction_data loads tables in dependency order (parent first)
- Transaction boundaries are NOT managed by loader (caller responsibility)
- Re-running extraction with same data produces no duplicates
- Atomic rollback verified: simulated failure leaves zero partial data
- ExtractionLog audit trail records every pipeline run
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-05-SUMMARY.md`
</output>
