# Phase 4: Data Dashboard - Research

**Researched:** 2026-02-05
**Domain:** Streamlit dashboard with PostgreSQL integration
**Confidence:** HIGH

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

#### Dashboard layout
- Split view on home page: top section with key metric cards, bottom section with main data table â€” both visible without scrolling
- Sidebar navigation with one tab per entity type (propostas, programas, apoiadores, emendas) plus a home/overview tab
- Default time range: last 7 days for operational data

#### Data exploration
- Cross-filtering: selecting a proposta auto-filters other sidebar tabs to show only related programas, apoiadores, emendas
- CSV export button on each data table for the current filtered view
- Data tables are browsable with search, sort, and filter

#### Operational visibility
- Extraction history covering last 30 days of pipeline runs (per success criteria) with default view of last 7 days
- Data freshness indicators showing last extraction date and time
- Pipeline run status (success/partial/failed) with row counts

### Claude's Discretion
- Information density per entity page (which columns visible by default vs expandable)
- Metrics style on overview (numbers only vs sparklines â€” based on available extraction history data)
- Search/filter approach per entity (basic text search vs column-specific â€” based on data structure)
- Sorting strategy (which columns are sortable â€” based on data types)
- Extraction history visualization format (table vs calendar heatmap)
- Pipeline health presentation (dedicated section vs integrated into overview cards)
- Time range control widget (date picker vs preset buttons)
- Visual style, color scheme, and chart types

### Deferred Ideas (OUT OF SCOPE)
None â€” discussion stayed within phase scope

</user_constraints>

## Summary

This research investigates how to build a Streamlit dashboard that visualizes Transfer Gov data from PostgreSQL. Streamlit 1.54.0 (latest as of Feb 2026) provides native PostgreSQL integration via `st.connection()`, built-in interactive dataframes with search/sort/filter, and multipage app support. The standard approach uses `st.navigation` for tab organization, `st.session_state` for cross-filtering between related entities, `st.metric` with sparklines for KPI cards, and `st.download_button` for CSV exports.

Railway deployment is straightforward â€” Streamlit auto-detects Python and runs on the PORT environment variable. Multiple services (FastAPI + Streamlit) can coexist in the same Railway project using separate service configurations in railway.json.

Portuguese UTF-8 characters are fully supported â€” Streamlit 1.54.0 includes UTF-8 encoding fixes for systems where UTF-8 is not the default, and PostgreSQL connections via psycopg3 handle UTF-8 correctly by default.

**Primary recommendation:** Use Streamlit 1.54.0 with st.navigation for multipage structure, st.session_state for cross-filtering state management, st.connection("postgresql") with 10-minute caching for database queries, and st.dataframe with built-in search/sort for data tables. Deploy as separate Railway service alongside existing FastAPI.

## Standard Stack

The established libraries/tools for this domain:

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| streamlit | 1.54.0 | Web dashboard framework | Official framework for data apps with built-in widgets, caching, state management, and deployment support |
| psycopg[binary] | 3.3.2+ | PostgreSQL adapter | Required by SQLAlchemy, already in project dependencies, handles UTF-8 correctly |
| sqlalchemy | 2.0.46+ | Database ORM | Already in project, reuse existing models and connection patterns |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| pandas | 2.2.0+ | DataFrame operations | Converting SQLAlchemy query results to dataframes for st.dataframe display |
| plotly | latest | Interactive charts | Optional: for custom visualizations beyond st.metric sparklines |
| polars | 1.38.0+ | High-performance data | Optional: if performance with large datasets (>150k rows) becomes an issue |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Streamlit | Dash (Plotly) | More boilerplate code, less built-in functionality, but more customizable styling |
| Streamlit | Gradio | Simpler but ML-focused, lacks multipage structure and state management features |
| st.navigation | pages/ directory | Simpler file-based routing but less flexibility for cross-filtering state sharing |

**Installation:**
```bash
# Add to pyproject.toml dependencies
streamlit>=1.54.0
```

## Architecture Patterns

### Recommended Project Structure
```
src/dashboard/
â”œâ”€â”€ streamlit_app.py       # Entrypoint: st.navigation, shared session state init
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ metrics.py         # Reusable KPI card components
â”‚   â”œâ”€â”€ filters.py         # Reusable filter widgets with callbacks
â”‚   â””â”€â”€ export.py          # CSV download button with caching
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ home.py            # Overview tab: metrics + recent propostas table
â”‚   â”œâ”€â”€ propostas.py       # Propostas detail tab with cross-filtering
â”‚   â”œâ”€â”€ programas.py       # Programas detail tab
â”‚   â”œâ”€â”€ apoiadores.py      # Apoiadores detail tab
â”‚   â””â”€â”€ emendas.py         # Emendas detail tab
â”œâ”€â”€ queries/
â”‚   â”œâ”€â”€ metrics.py         # Cached query functions for KPI metrics
â”‚   â”œâ”€â”€ entities.py        # Cached query functions per entity table
â”‚   â””â”€â”€ history.py         # Cached query for extraction_logs
â””â”€â”€ config.py              # Database connection setup
```

### Pattern 1: Database Connection with Secrets
**What:** Use Streamlit's st.connection for PostgreSQL with secrets management
**When to use:** All database access (replaces direct SQLAlchemy session usage)
**Example:**
```python
# Source: https://docs.streamlit.io/develop/tutorials/databases/postgresql
# .streamlit/secrets.toml (gitignored)
[connections.postgresql]
dialect = "postgresql"
host = "your-railway-host"
port = "5432"
database = "railway"
username = "postgres"
password = "xxx"

# src/dashboard/config.py
import streamlit as st

@st.cache_resource
def get_db_connection():
    """Get cached database connection."""
    return st.connection("postgresql", type="sql")

# Usage in query functions
conn = get_db_connection()
df = conn.query("SELECT * FROM propostas LIMIT 100", ttl="10m")
```

### Pattern 2: Multipage Navigation with Shared State
**What:** Use st.Page and st.navigation for tab structure with session_state for cross-filtering
**When to use:** Main app entrypoint to define all tabs and initialize shared state
**Example:**
```python
# Source: https://docs.streamlit.io/develop/concepts/multipage-apps/page-and-navigation
# src/dashboard/streamlit_app.py
import streamlit as st
from pages import home, propostas, programas, apoiadores, emendas

# Initialize session state for cross-filtering
if "selected_proposta_id" not in st.session_state:
    st.session_state.selected_proposta_id = None
if "time_range_days" not in st.session_state:
    st.session_state.time_range_days = 7  # Default to last 7 days

# Define pages
pages = {
    "Overview": st.Page(home, title="Overview", icon="ðŸ“Š"),
    "Entities": [
        st.Page(propostas, title="Propostas", icon="ðŸ“„"),
        st.Page(programas, title="Programas", icon="ðŸ›ï¸"),
        st.Page(apoiadores, title="Apoiadores", icon="ðŸ‘¥"),
        st.Page(emendas, title="Emendas", icon="ðŸ’°"),
    ],
}

# Navigation with sidebar
pg = st.navigation(pages, position="sidebar")
pg.run()
```

### Pattern 3: Cached Query Functions
**What:** Use @st.cache_data for database queries with time-to-live (TTL)
**When to use:** All database read operations to avoid repeated queries on rerun
**Example:**
```python
# Source: https://docs.streamlit.io/develop/tutorials/databases/postgresql
# src/dashboard/queries/metrics.py
import streamlit as st
import pandas as pd
from src.dashboard.config import get_db_connection

@st.cache_data(ttl="10m")  # Cache for 10 minutes
def get_entity_counts() -> pd.DataFrame:
    """Get row counts per entity table."""
    conn = get_db_connection()
    query = """
        SELECT
            'propostas' as entity, COUNT(*) as count FROM propostas
        UNION ALL
        SELECT 'programas', COUNT(*) FROM programas
        UNION ALL
        SELECT 'apoiadores', COUNT(*) FROM apoiadores
        UNION ALL
        SELECT 'emendas', COUNT(*) FROM emendas
    """
    return conn.query(query, ttl="10m")

@st.cache_data(ttl="10m")
def get_data_freshness() -> pd.DataFrame:
    """Get last extraction date per entity."""
    conn = get_db_connection()
    query = """
        SELECT
            'propostas' as entity,
            MAX(extraction_date) as last_extraction
        FROM propostas
        UNION ALL
        SELECT 'programas', MAX(extraction_date) FROM programas
        UNION ALL
        SELECT 'apoiadores', MAX(extraction_date) FROM apoiadores
        UNION ALL
        SELECT 'emendas', MAX(extraction_date) FROM emendas
    """
    return conn.query(query, ttl="10m")

@st.cache_data(ttl="10m")
def get_extraction_history(days: int = 30) -> pd.DataFrame:
    """Get pipeline run history for last N days."""
    conn = get_db_connection()
    query = f"""
        SELECT
            run_date,
            status,
            total_records,
            records_inserted,
            records_updated,
            duration_seconds
        FROM extraction_logs
        WHERE run_date >= CURRENT_DATE - INTERVAL '{days} days'
        ORDER BY run_date DESC
    """
    return conn.query(query, ttl="10m")
```

### Pattern 4: Cross-Filtering with Session State Callbacks
**What:** Store selected entity IDs in session_state and use callbacks to update filters
**When to use:** When selecting a row in one entity should filter related entities in other tabs
**Example:**
```python
# Source: https://docs.streamlit.io/develop/concepts/architecture/session-state
# src/dashboard/pages/propostas.py
import streamlit as st
from src.dashboard.queries.entities import get_propostas_filtered

def on_proposta_select():
    """Callback when user selects a proposta row."""
    # st.dataframe on_select returns dict with 'selection' key
    selection = st.session_state.proposta_selection
    if selection and selection['rows']:
        selected_idx = selection['rows'][0]
        # Get transfer_gov_id from the selected row
        st.session_state.selected_proposta_id = st.session_state.propostas_df.iloc[selected_idx]['transfer_gov_id']
    else:
        st.session_state.selected_proposta_id = None

# Query propostas
propostas_df = get_propostas_filtered(st.session_state.time_range_days)
st.session_state.propostas_df = propostas_df  # Store for callback access

# Display with row selection
st.dataframe(
    propostas_df,
    use_container_width=True,
    on_select=on_proposta_select,
    selection_mode="single-row",
    key="proposta_selection",
)

# src/dashboard/pages/apoiadores.py
import streamlit as st
from src.dashboard.queries.entities import get_apoiadores_for_proposta

# If a proposta is selected, filter apoiadores
if st.session_state.selected_proposta_id:
    st.info(f"Filtered by selected proposta: {st.session_state.selected_proposta_id}")
    apoiadores_df = get_apoiadores_for_proposta(st.session_state.selected_proposta_id)
else:
    apoiadores_df = get_all_apoiadores()

st.dataframe(apoiadores_df, use_container_width=True)
```

### Pattern 5: KPI Metrics with Sparklines
**What:** Use st.metric with chart_data parameter for trend visualization
**When to use:** Overview page to show key metrics with historical trends
**Example:**
```python
# Source: https://docs.streamlit.io/develop/api-reference/data/st.metric
# src/dashboard/pages/home.py
import streamlit as st
from src.dashboard.queries.metrics import get_entity_counts, get_extraction_history

# Layout: 4 columns for metrics
col1, col2, col3, col4 = st.columns(4)

counts = get_entity_counts()
history = get_extraction_history(days=30)

# Extract trend data (last 7 days of total_records)
recent_history = history.tail(7)
trend_data = recent_history['total_records'].tolist() if len(recent_history) > 0 else None

with col1:
    proposta_count = counts[counts['entity'] == 'propostas']['count'].values[0]
    st.metric(
        label="Propostas",
        value=f"{proposta_count:,}",
        delta=None,
        chart_data=trend_data,
        chart_type="line",
        border=True,
    )

with col2:
    programa_count = counts[counts['entity'] == 'programas']['count'].values[0]
    st.metric(
        label="Programas",
        value=f"{programa_count:,}",
        border=True,
    )
# ... similar for apoiadores and emendas
```

### Pattern 6: CSV Export with Caching
**What:** Use st.download_button with cached DataFrame-to-CSV conversion
**When to use:** Export functionality on each entity tab
**Example:**
```python
# Source: https://docs.streamlit.io/develop/api-reference/widgets/st.download_button
# src/dashboard/components/export.py
import streamlit as st
import pandas as pd

@st.cache_data
def convert_df_to_csv(df: pd.DataFrame) -> bytes:
    """Convert DataFrame to CSV with UTF-8 encoding."""
    return df.to_csv(index=False).encode("utf-8")

def add_csv_download(df: pd.DataFrame, filename: str):
    """Add CSV download button for given DataFrame."""
    csv_data = convert_df_to_csv(df)
    st.download_button(
        label="ðŸ“¥ Export to CSV",
        data=csv_data,
        file_name=filename,
        mime="text/csv",
        icon="ðŸ“¥",
    )

# Usage in pages/propostas.py
from src.dashboard.components.export import add_csv_download

propostas_df = get_propostas_filtered(st.session_state.time_range_days)
st.dataframe(propostas_df, use_container_width=True)
add_csv_download(propostas_df, "propostas_export.csv")
```

### Pattern 7: Railway Deployment Configuration
**What:** Configure Streamlit service in railway.json alongside FastAPI service
**When to use:** Deployment setup for Railway
**Example:**
```json
// Source: https://railway.com/deploy/streamlit
// railway.json
{
  "$schema": "https://railway.com/railway.schema.json",
  "build": {
    "builder": "NIXPACKS"
  },
  "deploy": {
    "startCommand": "streamlit run src/dashboard/streamlit_app.py --server.port=$PORT --server.address=0.0.0.0",
    "restartPolicyType": "ON_FAILURE",
    "restartPolicyMaxRetries": 10
  }
}
```

### Anti-Patterns to Avoid

- **Direct SQLAlchemy session in pages:** Use st.connection instead of get_session(). Streamlit's connection handles pooling and caching better than manual session management.

- **Querying without TTL:** Always set ttl parameter in conn.query() or @st.cache_data. Without TTL, stale data persists until app restart.

- **Mutating cached DataFrames:** Never modify DataFrames returned from @st.cache_data functions. Create a copy first: `df_filtered = cached_df.copy()`.

- **Accessing uninitialized session_state:** Always check `if "key" not in st.session_state` before accessing. Streamlit throws exceptions on undefined keys.

- **Using pages/ directory with cross-filtering:** The file-based pages/ approach doesn't support shared entrypoint code for session_state initialization. Use st.navigation instead.

- **Hardcoded database credentials:** Never commit .streamlit/secrets.toml. Use Railway environment variables and mount secrets at deployment.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Data table with search/sort/filter | Custom React table or manual filtering widgets | st.dataframe with built-in interactivity | Streamlit 1.54.0 has native column sorting, search bar, and filtering. Custom tables are 100+ lines of code. |
| CSV export | Custom file writing and download logic | st.download_button + df.to_csv() | Built-in widget handles MIME types, encoding, and browser download. Custom solutions miss edge cases. |
| Database connection pooling | Manual SQLAlchemy session management | st.connection("postgresql") | Handles connection pooling, retry logic, and caching automatically. Manual pooling risks connection leaks. |
| State persistence between tabs | Custom global variables or external store | st.session_state | Thread-safe per-user state management built-in. Custom global state breaks with multiple users. |
| Query result caching | Manual caching with dictionaries | @st.cache_data with TTL | Handles cache invalidation, memory management, and serialization. Manual caching is error-prone. |
| Multipage navigation | Custom routing with URL parameters | st.navigation with st.Page | Provides sidebar navigation, URL routing, and page lifecycle. Custom routing is 50+ lines. |

**Key insight:** Streamlit's built-in components handle complex edge cases (concurrent users, memory management, UTF-8 encoding, caching invalidation) that custom solutions typically miss until production. The framework's opinionated design reduces boilerplate by 70-80% compared to custom Flask/FastAPI dashboards.

## Common Pitfalls

### Pitfall 1: Forgetting Cache TTL Leads to Stale Data
**What goes wrong:** Dashboard shows outdated row counts and extraction status because queries cache indefinitely.
**Why it happens:** @st.cache_data without ttl parameter caches forever. Developers forget to add ttl="10m" to query functions.
**How to avoid:**
- Always specify ttl parameter: @st.cache_data(ttl="10m") for operational data
- Use shorter TTL (1-5 min) for extraction status, longer (10-30 min) for entity counts
- Document TTL choices in query function docstrings
**Warning signs:**
- Users report data freshness indicators don't update
- Pipeline runs complete but dashboard still shows "0 records"
- Metrics don't change even after manual database inserts

### Pitfall 2: Mutating Cached Objects Corrupts Data
**What goes wrong:** Filtering a DataFrame returned from @st.cache_data modifies the cached object, causing bugs for other users.
**Why it happens:** Python passes objects by reference. Modifying df.loc[] or df['col'] = value changes the cached DataFrame.
**How to avoid:**
- Always copy cached DataFrames before modifying: `df_filtered = cached_df.copy()`
- Use immutable operations: df.query() or df[df['col'] > 10] create new DataFrames
- Test with multiple browser tabs (simulates concurrent users)
**Warning signs:**
- Filters applied in one tab affect data in another tab
- Data disappears or changes unexpectedly between reruns
- Exceptions about "object is not writable"

### Pitfall 3: Uninitialized Session State Causes Exceptions
**What goes wrong:** Accessing st.session_state.selected_proposta_id before initialization throws KeyError.
**Why it happens:** Session state starts empty. Callbacks or page navigation can access keys before they're set.
**How to avoid:**
- Initialize all state keys in streamlit_app.py entrypoint before st.navigation
- Use defensive checks: `if "key" not in st.session_state: st.session_state.key = default_value`
- Never assume a key exists in page files
**Warning signs:**
- "KeyError: 'selected_proposta_id'" in logs
- App crashes on first load but works on reload
- Navigation between tabs throws exceptions

### Pitfall 4: Portuguese Characters Break CSV Downloads
**What goes wrong:** Downloaded CSV files show garbled characters (ï¿½) instead of Ã£, Ã§, Ãµ.
**Why it happens:** Forgetting .encode("utf-8") when converting DataFrame to bytes for st.download_button.
**How to avoid:**
- Always use: `df.to_csv(index=False).encode("utf-8")`
- Test CSV downloads with actual Portuguese text (tÃ­tulos, situaÃ§Ã£o, municÃ­pio)
- Open downloaded files in Excel and text editors to verify encoding
**Warning signs:**
- Users report unreadable municipality names
- TÃ­tulos display with ï¿½ characters in exported files
- CSV opens correctly in terminal but not Excel (Excel uses wrong encoding)

### Pitfall 5: Large Dataframes Block Reruns
**What goes wrong:** Displaying 50,000+ row propostas table makes the app unresponsive for 5-10 seconds on every rerun.
**Why it happens:** st.dataframe renders all rows even though users see only ~50 rows. No pagination by default.
**How to avoid:**
- Limit query results: `LIMIT 1000` in SQL for detail tables
- Implement pagination with st.selectbox for page selection
- Use st.dataframe height parameter: `st.dataframe(df, height=400)` to prevent rendering all rows
- Consider Streamlit AgGrid component for tables >10k rows
**Warning signs:**
- Spinner shows for >2 seconds when switching tabs
- Browser tab becomes unresponsive
- Memory usage grows with each rerun

### Pitfall 6: Secrets Not Available on Railway
**What goes wrong:** Deployed dashboard crashes with "Connection refused" because database credentials are missing.
**Why it happens:** .streamlit/secrets.toml is gitignored (correctly) but not configured in Railway environment variables.
**How to avoid:**
- Document required secrets in README: DATABASE_URL, etc.
- Use Railway's environment variables instead of secrets.toml for deployment
- Modify config.py to check for environment variables first, then fall back to secrets.toml
- Test deployment configuration locally with docker
**Warning signs:**
- App works locally but fails on Railway with connection errors
- Logs show "No such file or directory: .streamlit/secrets.toml"
- Database connection timeouts on deployed app

### Pitfall 7: Cross-Filtering Breaks on Page Navigation
**What goes wrong:** Selecting a proposta on the Propostas tab, then navigating to Apoiadores tab shows all apoiadores instead of filtered ones.
**Why it happens:** Session state is cleared or the callback doesn't persist selection before navigation.
**How to avoid:**
- Initialize all cross-filtering state keys in streamlit_app.py entrypoint
- Use on_select callbacks that update session_state immediately
- Debug with st.write(st.session_state) to verify state persists across pages
- Avoid using st.experimental_rerun() which can clear state
**Warning signs:**
- Filters work within a page but reset when changing tabs
- st.write(st.session_state) shows empty dict on page load
- Cross-filtering works intermittently (race condition)

### Pitfall 8: Automatic Column Sorting Disabled for Large Tables
**What goes wrong:** Users can't sort proposta table by valor_global because Streamlit disabled sorting.
**Why it happens:** Streamlit 1.54.0 automatically disables column sorting for DataFrames with >150,000 rows for performance.
**How to avoid:**
- Limit query results with SQL: `LIMIT 10000` for detail views
- Implement server-side sorting: add ORDER BY clause based on user selection
- Use st.selectbox for "Sort by" instead of relying on column headers
- Document sorting limitations in UI if tables exceed 150k rows
**Warning signs:**
- Column headers don't show sort arrows
- Clicking headers doesn't change row order
- Users report "sorting doesn't work" for large tables

## Code Examples

Verified patterns from official sources:

### Complete Home Page with Metrics and Table
```python
# Source: Streamlit official docs patterns
# src/dashboard/pages/home.py
import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
from src.dashboard.queries.metrics import (
    get_entity_counts,
    get_data_freshness,
    get_extraction_history,
)
from src.dashboard.queries.entities import get_recent_propostas
from src.dashboard.components.export import add_csv_download

st.title("ðŸ“Š Transfer Gov Dashboard")
st.caption("Operational visibility for pipeline extractions")

# Top section: Key metrics in 4 columns
st.subheader("Entity Counts")
col1, col2, col3, col4 = st.columns(4)

counts = get_entity_counts()

with col1:
    proposta_count = counts[counts['entity'] == 'propostas']['count'].values[0]
    st.metric(
        label="Propostas",
        value=f"{proposta_count:,}",
        border=True,
    )

with col2:
    programa_count = counts[counts['entity'] == 'programas']['count'].values[0]
    st.metric(
        label="Programas",
        value=f"{programa_count:,}",
        border=True,
    )

with col3:
    apoiador_count = counts[counts['entity'] == 'apoiadores']['count'].values[0]
    st.metric(
        label="Apoiadores",
        value=f"{apoiador_count:,}",
        border=True,
    )

with col4:
    emenda_count = counts[counts['entity'] == 'emendas']['count'].values[0]
    st.metric(
        label="Emendas",
        value=f"{emenda_count:,}",
        border=True,
    )

# Data freshness indicators
st.subheader("Data Freshness")
freshness = get_data_freshness()
for _, row in freshness.iterrows():
    entity = row['entity']
    last_extraction = row['last_extraction']
    if pd.notna(last_extraction):
        days_ago = (datetime.now().date() - last_extraction).days
        if days_ago == 0:
            status = "ðŸŸ¢ Today"
        elif days_ago <= 7:
            status = f"ðŸŸ¡ {days_ago} days ago"
        else:
            status = f"ðŸ”´ {days_ago} days ago"
        st.caption(f"{entity.capitalize()}: {status} ({last_extraction})")
    else:
        st.caption(f"{entity.capitalize()}: âšª No data")

st.divider()

# Bottom section: Recent propostas table
st.subheader("Recent Propostas (Last 7 Days)")

# Time range selector
time_range = st.selectbox(
    "Time range",
    options=[7, 14, 30],
    format_func=lambda x: f"Last {x} days",
    key="home_time_range",
)
st.session_state.time_range_days = time_range

recent_df = get_recent_propostas(days=time_range, limit=100)

if len(recent_df) > 0:
    # Display table with selection
    st.dataframe(
        recent_df,
        use_container_width=True,
        hide_index=True,
        column_config={
            "valor_global": st.column_config.NumberColumn(
                "Valor Global",
                format="R$ %.2f",
            ),
            "data_publicacao": st.column_config.DateColumn(
                "Data PublicaÃ§Ã£o",
                format="DD/MM/YYYY",
            ),
        },
    )

    # CSV export
    add_csv_download(recent_df, f"propostas_last_{time_range}_days.csv")
else:
    st.info(f"No propostas found in the last {time_range} days.")
```

### Extraction History with Pipeline Status
```python
# Source: Streamlit metrics and dataframe docs
# src/dashboard/pages/home.py (additional section)
st.divider()
st.subheader("Extraction History (Last 30 Days)")

history_df = get_extraction_history(days=30)

if len(history_df) > 0:
    # Show only last 7 days by default, with option to expand
    show_all = st.checkbox("Show all 30 days", value=False)

    display_df = history_df if show_all else history_df.head(7)

    # Add status badges
    def format_status(status):
        badges = {
            "success": "ðŸŸ¢ Success",
            "partial": "ðŸŸ¡ Partial",
            "failed": "ðŸ”´ Failed",
        }
        return badges.get(status, status)

    display_df['status_display'] = display_df['status'].apply(format_status)

    st.dataframe(
        display_df,
        use_container_width=True,
        hide_index=True,
        column_config={
            "run_date": st.column_config.DatetimeColumn(
                "Run Date",
                format="DD/MM/YYYY HH:mm",
            ),
            "status_display": st.column_config.TextColumn(
                "Status",
            ),
            "total_records": st.column_config.NumberColumn(
                "Total Records",
                format="%d",
            ),
            "duration_seconds": st.column_config.NumberColumn(
                "Duration",
                format="%.1f s",
            ),
        },
        column_order=[
            "run_date",
            "status_display",
            "total_records",
            "records_inserted",
            "records_updated",
            "duration_seconds",
        ],
    )
else:
    st.info("No extraction history available.")
```

### Cross-Filtering Query Functions
```python
# Source: PostgreSQL query patterns
# src/dashboard/queries/entities.py
import streamlit as st
import pandas as pd
from src.dashboard.config import get_db_connection

@st.cache_data(ttl="10m")
def get_apoiadores_for_proposta(proposta_id: str) -> pd.DataFrame:
    """Get apoiadores linked to a specific proposta via junction table."""
    conn = get_db_connection()
    query = """
        SELECT DISTINCT
            a.transfer_gov_id,
            a.nome,
            a.tipo,
            a.orgao,
            a.extraction_date
        FROM apoiadores a
        INNER JOIN proposta_apoiadores pa
            ON a.transfer_gov_id = pa.apoiador_transfer_gov_id
        WHERE pa.proposta_transfer_gov_id = %(proposta_id)s
        ORDER BY a.nome
    """
    return conn.query(query, params={"proposta_id": proposta_id}, ttl="10m")

@st.cache_data(ttl="10m")
def get_emendas_for_proposta(proposta_id: str) -> pd.DataFrame:
    """Get emendas linked to a specific proposta via junction table."""
    conn = get_db_connection()
    query = """
        SELECT DISTINCT
            e.transfer_gov_id,
            e.numero,
            e.autor,
            e.valor,
            e.tipo,
            e.ano,
            e.extraction_date
        FROM emendas e
        INNER JOIN proposta_emendas pe
            ON e.transfer_gov_id = pe.emenda_transfer_gov_id
        WHERE pe.proposta_transfer_gov_id = %(proposta_id)s
        ORDER BY e.ano DESC, e.numero
    """
    return conn.query(query, params={"proposta_id": proposta_id}, ttl="10m")

@st.cache_data(ttl="10m")
def get_programa_for_proposta(proposta_id: str) -> pd.DataFrame:
    """Get programa associated with a proposta (application-level FK)."""
    conn = get_db_connection()
    query = """
        SELECT
            prog.transfer_gov_id,
            prog.nome,
            prog.orgao_superior,
            prog.orgao_vinculado,
            prog.modalidade
        FROM programas prog
        INNER JOIN propostas prop
            ON prog.transfer_gov_id = prop.programa_id
        WHERE prop.transfer_gov_id = %(proposta_id)s
    """
    return conn.query(query, params={"proposta_id": proposta_id}, ttl="10m")
```

### Proposta Detail Page with Cross-Filtering
```python
# Source: Streamlit session state callbacks
# src/dashboard/pages/propostas.py
import streamlit as st
from src.dashboard.queries.entities import get_propostas_filtered
from src.dashboard.components.export import add_csv_download

st.title("ðŸ“„ Propostas")

# Clear selection button
if st.session_state.get("selected_proposta_id"):
    if st.button("ðŸ”„ Clear Selection"):
        st.session_state.selected_proposta_id = None
        st.rerun()

# Time range filter
time_range = st.selectbox(
    "Time range",
    options=[7, 14, 30, 90],
    format_func=lambda x: f"Last {x} days" if x else "All time",
    index=0,
)

# State filter
estados = st.multiselect(
    "Filter by Estado (UF)",
    options=["AC", "AL", "AP", "AM", "BA", "CE", "DF", "ES", "GO",
             "MA", "MT", "MS", "MG", "PA", "PB", "PR", "PE", "PI",
             "RJ", "RN", "RS", "RO", "RR", "SC", "SP", "SE", "TO"],
)

# Query propostas
propostas_df = get_propostas_filtered(
    days=time_range,
    estados=estados if estados else None,
)

st.caption(f"Showing {len(propostas_df)} propostas")

if len(propostas_df) > 0:
    # Callback for row selection
    def on_row_select():
        selection = st.session_state.proposta_table_selection
        if selection and 'rows' in selection and selection['rows']:
            selected_idx = selection['rows'][0]
            st.session_state.selected_proposta_id = propostas_df.iloc[selected_idx]['transfer_gov_id']
        else:
            st.session_state.selected_proposta_id = None

    # Display table
    st.dataframe(
        propostas_df,
        use_container_width=True,
        hide_index=True,
        on_select=on_row_select,
        selection_mode="single-row",
        key="proposta_table_selection",
        column_config={
            "valor_global": st.column_config.NumberColumn(
                "Valor Global",
                format="R$ %.2f",
            ),
            "valor_repasse": st.column_config.NumberColumn(
                "Valor Repasse",
                format="R$ %.2f",
            ),
            "data_publicacao": st.column_config.DateColumn(
                "Data PublicaÃ§Ã£o",
                format="DD/MM/YYYY",
            ),
            "situacao": st.column_config.TextColumn(
                "SituaÃ§Ã£o",
                width="medium",
            ),
        },
    )

    # CSV export
    add_csv_download(propostas_df, "propostas_filtered.csv")

    # Show selection info
    if st.session_state.get("selected_proposta_id"):
        st.success(f"âœ“ Selected: {st.session_state.selected_proposta_id} â€” Navigate to Apoiadores or Emendas tabs to see related data")
else:
    st.info("No propostas found for the selected filters.")
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| @st.cache decorator | @st.cache_data and @st.cache_resource | Streamlit 1.18 (2022) | Separate data caching from resource caching, clearer semantics |
| pages/ directory only | st.navigation with st.Page | Streamlit 1.37 (2024) | Dynamic page lists, custom navigation UI, shared entrypoint code |
| Manual secrets loading | st.connection with secrets.toml | Streamlit 1.28 (2023) | Automatic credential management, connection pooling, retry logic |
| st.dataframe without sorting | Built-in column sorting | Streamlit 1.35 (2024) | No need for custom sorting widgets, but auto-disabled >150k rows |
| Plain st.metric | st.metric with sparklines | Streamlit 1.52 (2025) | Visual trend indicators without separate charts |
| UTF-8 encoding issues | Explicit UTF-8 in streamlit init | Streamlit 1.54 (2026) | Portuguese characters work by default on all systems |

**Deprecated/outdated:**
- **@st.cache:** Replaced by @st.cache_data and @st.cache_resource in 1.18. Use the new decorators for better performance and semantics.
- **pages/ directory for complex apps:** While still supported, st.navigation is preferred for apps with cross-page state or dynamic navigation (recommended in 1.37+).
- **Manual connection pooling:** st.connection handles this automatically since 1.28. Don't use manual SQLAlchemy session factories.
- **st.experimental_rerun():** Replaced by st.rerun() in 1.27. The experimental version is deprecated.

## Open Questions

Things that couldn't be fully resolved:

1. **Optimal TTL for extraction_logs queries**
   - What we know: 10 minutes is standard for operational data, 1 minute for real-time dashboards
   - What's unclear: Whether pipeline runs frequently enough to justify 1-min TTL or if 10-min is sufficient
   - Recommendation: Start with 10-min TTL, add refresh button for manual cache clearing if users need immediate updates

2. **Pagination strategy for large proposta tables**
   - What we know: st.dataframe auto-disables sorting at 150k rows, becomes slow at 50k+ rows
   - What's unclear: Current proposta count and expected growth rate
   - Recommendation: Implement SQL LIMIT 1000 initially, add pagination if users request it. Monitor performance with actual data size.

3. **Calendar heatmap library choice for extraction history**
   - What we know: July package exists for calendar heatmaps in Streamlit, Plotly can do month-hour heatmaps
   - What's unclear: Whether calendar view adds value over simple table, user preference
   - Recommendation: Start with st.dataframe table (simpler), collect user feedback, add heatmap visualization if requested

4. **Column visibility defaults for entity tables**
   - What we know: Database has 10-15 columns per entity (propostas has titulo, valor_global, valor_repasse, situacao, estado, municipio, proponente, etc.)
   - What's unclear: Which columns users prioritize for overview vs drill-down
   - Recommendation: Show 5-7 key columns initially (transfer_gov_id, nome/titulo, valor, situacao, data), use st.dataframe column_config to hide others. Add "Show all columns" checkbox.

5. **Multi-service Railway deployment configuration**
   - What we know: Railway supports multiple services in one project via railway.json or separate service configs
   - What's unclear: Whether to deploy as separate Railway service or add Streamlit to existing FastAPI Dockerfile
   - Recommendation: Deploy as separate Railway service with its own railway.json. Simpler than multi-process Docker, independent scaling, and cleaner separation.

## Sources

### Primary (HIGH confidence)
- [Streamlit PostgreSQL Tutorial](https://docs.streamlit.io/develop/tutorials/databases/postgresql) - Connection patterns, caching, secrets management
- [st.dataframe API Reference](https://docs.streamlit.io/develop/api-reference/data/st.dataframe) - Interactive features, column config, selection modes
- [Streamlit Session State Docs](https://docs.streamlit.io/develop/concepts/architecture/session-state) - Initialization patterns, callbacks, limitations
- [st.metric API Reference](https://docs.streamlit.io/develop/api-reference/data/st.metric) - Sparkline parameters, delta colors, formatting
- [st.download_button API Reference](https://docs.streamlit.io/develop/api-reference/widgets/st.download_button) - CSV export pattern, caching
- [Streamlit Multipage Apps](https://docs.streamlit.io/develop/concepts/multipage-apps/page-and-navigation) - st.navigation vs pages/ directory, project structure
- [Streamlit 1.54.0 Release (PyPI)](https://pypi.org/project/streamlit/) - Latest stable version
- [Streamlit 2026 Release Notes](https://docs.streamlit.io/develop/quick-reference/release-notes/2026) - UTF-8 encoding fixes, performance updates

### Secondary (MEDIUM confidence)
- [Streamlit PostgreSQL Best Practices](https://www.w3resource.com/PostgreSQL/snippets/streamlit-postgres.php) - Connection pooling guidance
- [Railway Streamlit Deployment](https://railway.com/deploy/streamlit) - Deployment configuration patterns
- [Streamlit Dashboard Patterns (Ploomber)](https://ploomber.io/blog/streamlit-postgres/) - Real-time data display patterns
- [Building KPI Dashboards in Streamlit](https://medium.com/@cameronjosephjones/building-a-kpi-dashboard-in-streamlit-using-python-c88ac63903f5) - Metrics layout patterns
- [Streamlit Performance with Large Datasets](https://www.comparepriceacross.com/post/master_large_datasets_for_peak_performance_in_streamlit/) - Pagination and optimization strategies

### Tertiary (LOW confidence)
- Community discussions on cross-filtering patterns - Implementation varies by use case
- Blog posts on Streamlit + SQLAlchemy patterns - Not all use st.connection
- Railway multi-service deployment examples - Limited official documentation for Python multi-service

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Streamlit 1.54.0 verified on PyPI, official PostgreSQL connection docs confirmed, project already uses SQLAlchemy
- Architecture: HIGH - All patterns from official Streamlit documentation, verified with source URLs
- Pitfalls: MEDIUM-HIGH - Most pitfalls from official docs and release notes, some from community patterns (caching mutations, UTF-8 encoding)

**Research date:** 2026-02-05
**Valid until:** 2026-04-05 (60 days - Streamlit is stable, monthly releases but mostly incremental features)
